# DeepLearning_Mini_Project

## Repository Structure

This repository is organized into four main branches, each serving a specific purpose in the project workflow:  
### Data Preprocessing and Dataset Generation Branch:
This branch contains all the scripts and notebooks used for data
preprocessing and generation of the dataset. It includes the code for processing the data, and
transforming it into a suitable dataset format for model training.

### Text Classification Baseline Training Branch: 
This branch is dedicated to the training of the text classification
baseline model. It includes the code for model architecture, training, and evaluation. The model trained in this branch
serves as a baseline for comparison with the audio classification models.

### Wav2Vec2 Audio Classification Model Training Branch:
This branch contains the code for training the Wav2Vec2 audio
classification model. It includes the code for model architecture, training, and evaluation. The Wav2Vec2 model is one
of the audio classification models used in this project.

### AST Audio Classification Model Training Branch:
This branch is dedicated to the training of the Audio Spectrogram
Transformer (AST) model. It includes the code for model architecture, training, and evaluation. The AST model is another
audio classification model used in this project.

Each branch is independent and contains all the necessary code and documentation for the specific task it is designed
for. Please switch to the appropriate branch for the task you are interested in.

## Abstract
In this study, we compare hate speech classification in audio using waveform and spectrogram-based approaches. Our
hypothesis suggests that detecting hate speech in audio files, generated by text-to-speech synthesis (TTS) of social
media comments from the dataset "Measuring-hate-speech", is viable. We aim to determine if models using spectrograms
outperform those using raw audio waveforms. To achieve this, we utilize the Coqui-TTS project for audio sample
generation. We fine-tune two models, wav2vec2 and the audio spectrogram transformer (AST), using the generated samples.
Additionally, we fine-tune a DistilBERT model trained on the non-synthesized text samples for comparison. Our evaluation
on validation and test sets showcases the potential of audio-based hate speech classification. Overall, our findings
underscore the effectiveness of audio-based hate speech classification methods and highlight a promising area for
further research possibilities.
