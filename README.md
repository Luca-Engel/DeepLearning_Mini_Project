# DeepLearning_Mini_Project

In this study, we compare hate speech classification in audio using waveform and spectrogram-based approaches. Our
hypothesis suggests that detecting hate speech in audio files, generated by text-to-speech synthesis (TTS) of social
media comments from the dataset "Measuring-hate-speech", is viable. We aim to determine if models using spectrograms
outperform those using raw audio waveforms. To achieve this, we utilize the Coqui-TTS project for audio sample
generation. We fine-tune two models, wav2vec2 and the audio spectrogram transformer (AST), using the generated samples.
Additionally, we fine-tune a DistilBERT model trained on the non-synthesized text samples for comparison. Our evaluation
on validation and test sets showcases the potential of audio-based hate speech classification. Overall, our findings
underscore the effectiveness of audio-based hate speech classification methods and highlight a promising area for
further research possibilities.